{"sample_idx": 6, "start_block_idx": 670, "last_block_idx": 776, "block_list": [{"block_idx": 0, "token_num": 41, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "So I think first of all, attention, like Yoshua Bengio wrote this paper with Dzmitry Bahdanau called, Soft Attention, which was first applied in this paper called Align and Translate."}, {"block_idx": 1, "token_num": 37, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Ilya Sutskever wrote the first paper that said, you can just train a simple RNN model, scale it up and it'll beat all the phrase-based machine translation systems."}, {"block_idx": 2, "token_num": 6, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But that was brute force."}, {"block_idx": 3, "token_num": 32, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "There was no attention in it, and spent a lot of Google compute, I think probably like 400 million parameter model or something even back in those days."}, {"block_idx": 4, "token_num": 34, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And then this grad student Bahdanau in Benjio's lab identifies attention and beats his numbers with [inaudible 01:05:20] compute."}, {"block_idx": 5, "token_num": 6, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "So clearly a great idea."}, {"block_idx": 6, "token_num": 35, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And then people at DeepMind figured that this paper called Pixel RNNs figured that you don't even need RNNs, even though the title is called Pixel RNN."}, {"block_idx": 7, "token_num": 13, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "I guess it's the actual architecture that became popular was WaveNet."}, {"block_idx": 8, "token_num": 24, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And they figured out that a completely convolutional model can do autoregressive modeling as long as you do mass convolutions."}, {"block_idx": 9, "token_num": 7, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "The masking was the key idea."}, {"block_idx": 10, "token_num": 15, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "So you can train in parallel instead of backpropagating through time."}, {"block_idx": 11, "token_num": 12, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "You can backpropagate through every input token in parallel."}, {"block_idx": 12, "token_num": 21, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "So that way you can utilize the GPU computer a lot more efficiently, because you're just doing Matmos."}, {"block_idx": 13, "token_num": 11, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And so they just said throw away the RNN."}, {"block_idx": 14, "token_num": 5, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And that was powerful."}, {"block_idx": 15, "token_num": 25, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And so then Google Brain, like Vaswani et al that transformer paper identified that, let's take the good elements of both."}, {"block_idx": 16, "token_num": 10, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Let's take attention, it's more powerful than cons."}, {"block_idx": 17, "token_num": 15, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "It learns more higher-order dependencies, because it applies more multiplicative compute."}, {"block_idx": 18, "token_num": 35, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And let's take the insight in WaveNet that you can just have a all convolutional model that fully parallel matrix multiplies and combine the two together and they built a transformer."}, {"block_idx": 19, "token_num": 16, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And that is the, I would say, it's almost like the last answer."}, {"block_idx": 20, "token_num": 28, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Nothing has changed since 2017 except maybe a few changes on what the nonlinearities are and how the square descaling should be done."}, {"block_idx": 21, "token_num": 6, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Some of that has changed."}, {"block_idx": 22, "token_num": 20, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And then people have tried mixture of experts having more parameters for the same flop and things like that."}, {"block_idx": 23, "token_num": 9, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But the core transformer architecture has not changed."}, {"block_idx": 24, "token_num": 19, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Isn't it crazy to you that masking as simple as something like that works so damn well?"}, {"block_idx": 25, "token_num": 36, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Yeah, it's a very clever insight that, you want to learn causal dependencies, but you don't want to waste your hardware, your compute and keep doing the back propagation sequentially."}, {"block_idx": 26, "token_num": 13, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "You want to do as much parallel compute as possible during training."}, {"block_idx": 27, "token_num": 18, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "That way, whatever job was earlier running in eight days would run in a single day."}, {"block_idx": 28, "token_num": 9, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "I think that was the most important insight."}, {"block_idx": 29, "token_num": 7, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And whether it's cons or attentionâ€¦"}, {"block_idx": 30, "token_num": 22, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "I guess attention and transformers make even better use of hardware than cons, because they apply more compute per flop."}, {"block_idx": 31, "token_num": 14, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Because in a transformer the self-attention operator doesn't even have parameters."}, {"block_idx": 32, "token_num": 21, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "The QK transpose softmax times V has no parameter, but it's doing a lot of flops."}, {"block_idx": 33, "token_num": 4, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And that's powerful."}, {"block_idx": 34, "token_num": 6, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "It learns multi-order dependencies."}, {"block_idx": 35, "token_num": 29, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "I think the insight then OpenAI took from that is, like Ilya Sutskever has been saying unsupervised learning is important."}, {"block_idx": 36, "token_num": 26, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "They wrote this paper called Sentiment Neuron, and then Alec Radford and him worked on this paper called GPT-1."}, {"block_idx": 37, "token_num": 14, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "It wasn't even called GPT-1, it was just called GPT."}, {"block_idx": 38, "token_num": 14, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Little did they know that it would go on to be this big."}, {"block_idx": 39, "token_num": 61, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But just said, let's revisit the idea that you can just train a giant language model and it'll learn natural language common sense, that was not scalable earlier because you were scaling up RNNs, but now you got this new transformer model that's a 100x more efficient at getting to the same performance."}, {"block_idx": 40, "token_num": 25, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Which means if you run the same job, you would get something that's way better if you apply the same amount of compute."}, {"block_idx": 41, "token_num": 24, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And so they just trained transformer on all the books like storybooks, children's storybooks, and that got really good."}, {"block_idx": 42, "token_num": 31, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And then Google took that inside and did BERT, except they did bidirectional, but they trained on Wikipedia and books and that got a lot better."}, {"block_idx": 43, "token_num": 13, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And then OpenAI followed up and said, okay, great."}, {"block_idx": 44, "token_num": 18, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "So it looks like the secret sauce that we were missing was data and throwing more parameters."}, {"block_idx": 45, "token_num": 25, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "So we'll get GPT-2, which is like a billion parameter model, and trained on a lot of links from Reddit."}, {"block_idx": 46, "token_num": 6, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And then that became amazing."}, {"block_idx": 47, "token_num": 16, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Produce all these stories about a unicorn and things like that, if you remember."}, {"block_idx": 48, "token_num": 2, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Yeah."}, {"block_idx": 49, "token_num": 19, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And then the GPT-3 happened, which is like you just scale up even more data."}, {"block_idx": 50, "token_num": 18, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "You take common crawl and instead of one billion go all the way to 175 billion."}, {"block_idx": 51, "token_num": 37, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But that was done through analysis called the scaling laws, which is, for a bigger model, you need to keep scaling the amount of tokens and you train on 300 billion tokens."}, {"block_idx": 52, "token_num": 5, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Now it feels small."}, {"block_idx": 53, "token_num": 18, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "These models are being trained on tens of trillions of tokens and trillions of parameters."}, {"block_idx": 54, "token_num": 7, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But this is literally the evolution."}, {"block_idx": 55, "token_num": 38, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Then the focus went more into pieces outside the architecture on data, what data you're training on, what are the tokens, how dedupe they are, and then the Chinchilla insight."}, {"block_idx": 56, "token_num": 20, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "It's not just about making the model bigger, but you want to also make the data set bigger."}, {"block_idx": 57, "token_num": 29, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "You want to make sure the tokens are also big enough in quantity and high quality and do the right evals on a lot of reasoning benchmarks."}, {"block_idx": 58, "token_num": 10, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "So I think that ended up being the breakthrough."}, {"block_idx": 59, "token_num": 9, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "It's not like a attention alone was important."}, {"block_idx": 60, "token_num": 25, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Attention, parallel computation, transformer, scaling it up to do unsupervised pre-training, right data and then constant improvements."}, {"block_idx": 61, "token_num": 31, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Well, let's take it to the end, because you just gave an epic history of LLMs and the breakthroughs of the past 10 years plus."}, {"block_idx": 62, "token_num": 12, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "So you mentioned GPT-3, so three, five."}, {"block_idx": 63, "token_num": 13, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "How important to you is RLHF, that aspect of it?"}, {"block_idx": 64, "token_num": 16, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "It's really important, even though you call it as a cherry on the cake."}, {"block_idx": 65, "token_num": 12, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "This cake has a lot of cherries, by the way."}, {"block_idx": 66, "token_num": 20, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "It's not easy to make these systems controllable and well-behaved without the RLHF step."}, {"block_idx": 67, "token_num": 10, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "By the way, there's this terminology for this."}, {"block_idx": 68, "token_num": 18, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "It's not very used in papers, but people talk about it as pre-trained post-trained."}, {"block_idx": 69, "token_num": 15, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And RLHF and supervised fine-tuning are all in post-training phase."}, {"block_idx": 70, "token_num": 12, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And the pre-training phase is the raw scaling on compute."}, {"block_idx": 71, "token_num": 15, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And without good post-training, you're not going to have a good product."}, {"block_idx": 72, "token_num": 26, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But at the same time, without good pre-training, there's not enough common sense to actually have the post-training have any effect."}, {"block_idx": 73, "token_num": 22, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "You can only teach a generally intelligent person a lot of skills, and that's where the pre-training is important."}, {"block_idx": 74, "token_num": 8, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "That's why you make the model bigger."}, {"block_idx": 75, "token_num": 27, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "The same RLHF on the bigger model ends up like GPT-4 ends up making ChatGPT much better than 3.5."}, {"block_idx": 76, "token_num": 34, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But that data like, oh, for this coding query, make sure the answer is formatted with these markdown and syntax highlighting tool use and knows when to use what tools."}, {"block_idx": 77, "token_num": 9, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "We can decompose the query into pieces."}, {"block_idx": 78, "token_num": 53, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "These are all stuff you do in the post-training phase, and that's what allows you to build products that users can interact with, collect more data, create a flywheel, go and look at all the cases where it's failing, collect more human annotation on that."}, {"block_idx": 79, "token_num": 12, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "I think that's where a lot more breakthroughs will be made."}, {"block_idx": 80, "token_num": 6, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "On the post-training side."}, {"block_idx": 81, "token_num": 2, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Yeah."}, {"block_idx": 82, "token_num": 5, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Post-training plus plus."}, {"block_idx": 83, "token_num": 20, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "So not just the training part of post-training, but a bunch of other details around that also."}, {"block_idx": 84, "token_num": 12, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And the RAG architecture, the Retrieval Augmented architecture."}, {"block_idx": 85, "token_num": 35, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "I think there's an interesting thought experiment here that, we've been spending a lot of compute in the pre-training to acquire general common sense, but that seems brute force and inefficient."}, {"block_idx": 86, "token_num": 15, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "What you want is a system that can learn like an open book exam."}, {"block_idx": 87, "token_num": 44, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "If you've written exams in undergrad or grad school where people allowed you to come with your notes to the exam, versus no notes allowed, I think not the same set of people end up scoring number one on both."}, {"block_idx": 88, "token_num": 9, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "You're saying pre-training is no notes allowed?"}, {"block_idx": 89, "token_num": 3, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Kind of."}, {"block_idx": 90, "token_num": 5, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "It memorizes everything."}, {"block_idx": 91, "token_num": 21, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "You can ask the question, why do you need to memorize every single fact to be good at reasoning?"}, {"block_idx": 92, "token_num": 24, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But somehow that seems like the more and more compute and data you throw at these models, they get better at reasoning."}, {"block_idx": 93, "token_num": 12, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But is there a way to decouple reasoning from facts?"}, {"block_idx": 94, "token_num": 25, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And there are some interesting research directions here, like Microsoft has been working on this five models where they're training small language models."}, {"block_idx": 95, "token_num": 20, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "They call it SLMs, but they're only training it on tokens that are important for reasoning."}, {"block_idx": 96, "token_num": 46, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And they're distilling the intelligence from GPT-4 on it to see how far you can get if you just take the tokens of GPT-4 on datasets that require you to reason, and you train the model only on that."}, {"block_idx": 97, "token_num": 21, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "You don't need to train on all of regular internet pages, just train it on basic common sense stuff."}, {"block_idx": 98, "token_num": 12, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But it's hard to know what tokens are needed for that."}, {"block_idx": 99, "token_num": 12, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "It's hard to know if there's an exhaustive set for that."}, {"block_idx": 100, "token_num": 48, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "But if we do manage to somehow get to a right dataset mix that gives good reasoning skills for a small model, then that's a breakthrough that disrupts the whole foundation model players, because you no longer need that giant of cluster for training."}, {"block_idx": 101, "token_num": 51, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "And if this small model, which has good level of common sense can be applied iteratively, it bootstraps its own reasoning and doesn't necessarily come up with one output answer, but things for a while bootstraps to calm things for a while."}, {"block_idx": 102, "token_num": 8, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "I think that can be truly transformational."}, {"block_idx": 103, "token_num": 9, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Man, there's a lot of questions there."}, {"block_idx": 104, "token_num": 9, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Is it possible to form that SLM?"}, {"block_idx": 105, "token_num": 23, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "You can use an LLM to help with the filtering which pieces of data are likely to be useful for reasoning?"}, {"block_idx": 106, "token_num": 2, "speaker_id": "speaker_0", "speaker_name": "Aravind Srinivas", "text": "Absolutely."}]}