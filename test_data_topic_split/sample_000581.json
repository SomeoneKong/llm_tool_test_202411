{"sample_idx": 12, "start_block_idx": 1194, "last_block_idx": 1322, "block_list": [{"block_idx": 0, "token_num": 13, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "What about RL with feedback side RLHF versus RLAIF?"}, {"block_idx": 1, "token_num": 13, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "What's the role of that in getting better performance on the models?"}, {"block_idx": 2, "token_num": 2, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Yeah."}, {"block_idx": 3, "token_num": 22, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "So RLHF is when the reward model you use is trained from some labels you've collected from humans giving feedback."}, {"block_idx": 4, "token_num": 26, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I think this works if you have the ability to get a ton of human feedback for this kind of task that you care about."}, {"block_idx": 5, "token_num": 30, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "RLAIF is interesting because you're depending on… This is actually, it's depending on the constraint that verification is actually a decent bit easier than generation."}, {"block_idx": 6, "token_num": 12, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Because it feels like, okay, what are you doing?"}, {"block_idx": 7, "token_num": 20, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Are you using this language model to look at the language model outputs and then prove the language model?"}, {"block_idx": 8, "token_num": 25, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "But no, it actually may work if the language model has a much easier time verifying some solution than it does generating it."}, {"block_idx": 9, "token_num": 12, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Then you actually could perhaps get this kind of recursive loop."}, {"block_idx": 10, "token_num": 12, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "But I don't think it's going to look exactly like that."}, {"block_idx": 11, "token_num": 56, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "The other thing you could do, that we kind of do, is a little bit of a mix of RLAIF and RLHF, where usually the model is actually quite correct and this is the case of precursor tap picking between two possible generations of what is the better one."}, {"block_idx": 12, "token_num": 37, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And then it just needs a little bit of human nudging with only on the order 50, 100 examples to align that prior the model has with exactly with what you want."}, {"block_idx": 13, "token_num": 21, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "It looks different than I think normal RLHF where you're usually training these reward models in tons of examples."}, {"block_idx": 14, "token_num": 14, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "What's your intuition when you compare generation and verification or generation and ranking?"}, {"block_idx": 15, "token_num": 7, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Is ranking way easier than generation?"}, {"block_idx": 16, "token_num": 12, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "My intuition would just say, yeah, it should be."}, {"block_idx": 17, "token_num": 6, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "This is going back to…"}, {"block_idx": 18, "token_num": 34, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Like, if you believe P does not equal NP, then there's this massive class of problems that are much, much easier to verify given proof, than actually proving it."}, {"block_idx": 19, "token_num": 19, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "I wonder if the same thing will prove P not equal to NP or P equal to NP."}, {"block_idx": 20, "token_num": 6, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "That would be really cool."}, {"block_idx": 21, "token_num": 11, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "That'd be a whatever Field's Medal by AI."}, {"block_idx": 22, "token_num": 5, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Who gets the credit?"}, {"block_idx": 23, "token_num": 6, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Another the open philosophical question."}, {"block_idx": 24, "token_num": 5, "speaker_id": "speaker_1", "speaker_name": "Michael", "text": "Whoever prompted it."}, {"block_idx": 25, "token_num": 20, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I'm actually surprisingly curious what a good bet for one AI will get the Field's Medal will be."}, {"block_idx": 26, "token_num": 5, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I actually don't have-"}, {"block_idx": 27, "token_num": 7, "speaker_id": "speaker_1", "speaker_name": "Michael", "text": "Isn't this Aman's specialty?"}, {"block_idx": 28, "token_num": 10, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I don't know what Aman's bet here is."}, {"block_idx": 29, "token_num": 12, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Oh, sorry, Nobel Prize or Field's Medal first?"}, {"block_idx": 30, "token_num": 4, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "Field's Medal-"}, {"block_idx": 31, "token_num": 7, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Oh, Field's Medal level?"}, {"block_idx": 32, "token_num": 9, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "Field's Medal comes first, I think."}, {"block_idx": 33, "token_num": 11, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "[inaudible 02:06:41]."}, {"block_idx": 34, "token_num": 5, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Fields Medal comes first."}, {"block_idx": 35, "token_num": 10, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Well, you would say that, of course."}, {"block_idx": 36, "token_num": 10, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "But it's also this isolated system you verify and…"}, {"block_idx": 37, "token_num": 2, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Sure."}, {"block_idx": 38, "token_num": 7, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I don't even know if I-"}, {"block_idx": 39, "token_num": 16, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "You don't need to do [inaudible 02:06:50]."}, {"block_idx": 40, "token_num": 11, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I feel like I have much more to do there."}, {"block_idx": 41, "token_num": 16, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "It felt like the path to get to IMO was a little bit more clear."}, {"block_idx": 42, "token_num": 34, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Because it already could get a few IMO problems and there was a bunch of low-hanging fruit, given the literature at the time, of what tactics people could take."}, {"block_idx": 43, "token_num": 17, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I think I'm, one, much less versed in the space of theorem proving now."}, {"block_idx": 44, "token_num": 20, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And two, less intuition about how close we are to solving these really, really hard open problems."}, {"block_idx": 45, "token_num": 10, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So you think you'll be Field's Medal first?"}, {"block_idx": 46, "token_num": 8, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "It won't be in physics or in-"}, {"block_idx": 47, "token_num": 5, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "Oh, 100%."}, {"block_idx": 48, "token_num": 7, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I think that's probably more likely."}, {"block_idx": 49, "token_num": 11, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "It is probably much more likely that it'll get in."}, {"block_idx": 50, "token_num": 2, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "Yeah,"}, {"block_idx": 51, "token_num": 2, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "yeah,"}, {"block_idx": 52, "token_num": 2, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "yeah."}, {"block_idx": 53, "token_num": 7, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "Well I think it both to…"}, {"block_idx": 54, "token_num": 40, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I don't know, BSD, which is a Birch and Swinnerton-Dyer conjecture, or Hodge Conjecture, or any one of these hard math problems are just actually really hard."}, {"block_idx": 55, "token_num": 15, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "It's sort of unclear what the path to get even a solution looks like."}, {"block_idx": 56, "token_num": 12, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "We don't even know what a path looks like, let alone"}, {"block_idx": 57, "token_num": 11, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "[inaudible 02:07:47]."}, {"block_idx": 58, "token_num": 34, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "And you don't buy the idea this is just like an isolated system and you can actually have a good reward system, and it feels like it's easier to train for that."}, {"block_idx": 59, "token_num": 12, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I think we might get Field's Medal before AGI."}, {"block_idx": 60, "token_num": 8, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I mean, I'd be very happy."}, {"block_idx": 61, "token_num": 5, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I'd be very happy."}, {"block_idx": 62, "token_num": 17, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "But I don't know if I… I think 2028, 2030."}, {"block_idx": 63, "token_num": 5, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "For Field's Medal?"}, {"block_idx": 64, "token_num": 4, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "Field's Medal."}, {"block_idx": 65, "token_num": 3, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "All right."}, {"block_idx": 66, "token_num": 15, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "It feels like forever from now, given how fast things have been going."}, {"block_idx": 67, "token_num": 15, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Speaking of how fast things have been going, let's talk about scaling laws."}, {"block_idx": 68, "token_num": 20, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So for people who don't know, maybe it's good to talk about this whole idea of scaling laws."}, {"block_idx": 69, "token_num": 19, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "What are they, where'd you think stand, and where do you think things are going?"}, {"block_idx": 70, "token_num": 6, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I think it was interesting."}, {"block_idx": 71, "token_num": 12, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "The original scaling laws paper by OpenAI was slightly wrong."}, {"block_idx": 72, "token_num": 13, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Because I think of some issues they did with learning right schedules."}, {"block_idx": 73, "token_num": 11, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And then Chinchilla proposed a more correct version."}, {"block_idx": 74, "token_num": 16, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And then from then people have again deviated from doing the compute optimal thing."}, {"block_idx": 75, "token_num": 19, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Because people start now optimizing more so for making the thing work really well given an inference budget."}, {"block_idx": 76, "token_num": 27, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And I think there are a lot more dimensions to these curves than what we originally used, of just compute number of parameters and data."}, {"block_idx": 77, "token_num": 8, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Like inference compute is the obvious one."}, {"block_idx": 78, "token_num": 9, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I think context length is another obvious one."}, {"block_idx": 79, "token_num": 32, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "So let's say you care about the two things of inference compute and then context window, maybe the thing you want to train, is some kind of SSM."}, {"block_idx": 80, "token_num": 15, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Because they're much, much cheaper and faster at super, super long context."}, {"block_idx": 81, "token_num": 40, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And even if, maybe it was 10 X more scaling properties during training, meaning you spend 10 X more compute to train the thing to get the same level of capabilities, it's worth it."}, {"block_idx": 82, "token_num": 14, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Because you care most about that inference budget for really long context windows."}, {"block_idx": 83, "token_num": 14, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "So it'll be interesting to see how people play with all these dimensions."}, {"block_idx": 84, "token_num": 14, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So yeah, I mean you speak to the multiple dimensions, obviously."}, {"block_idx": 85, "token_num": 43, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "The original conception was just looking at the variables of the size of the model as measured by parameters, and the size of the data as measured by the number of tokens, and looking at the ratio of the two."}, {"block_idx": 86, "token_num": 2, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Yeah."}, {"block_idx": 87, "token_num": 19, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "And it's kind of a compelling notion that there is a number, or at least a minimum."}, {"block_idx": 88, "token_num": 8, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "And it seems like one was emerging."}, {"block_idx": 89, "token_num": 14, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Do you still believe that there is a kind of bigger is better?"}, {"block_idx": 90, "token_num": 13, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I mean I think bigger is certainly better for just raw performance."}, {"block_idx": 91, "token_num": 4, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "And raw intelligence."}, {"block_idx": 92, "token_num": 4, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And raw intelligence."}, {"block_idx": 93, "token_num": 18, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I think the path that people might take, is… I'm particularly bullish on distillation."}, {"block_idx": 94, "token_num": 28, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And how many knobs can you turn to, if we spend a ton, ton of money on training, get the most capable cheap model."}, {"block_idx": 95, "token_num": 10, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Really, really caring as much as you can."}, {"block_idx": 96, "token_num": 28, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Because the naive version of caring as much as you can about inference time compute, is what people have already done with the Llama models."}, {"block_idx": 97, "token_num": 25, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Or just over-training the shit out of 7B models on way, way, way more tokens than is essential optimal."}, {"block_idx": 98, "token_num": 48, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "But if you really care about it, maybe the thing to do is what Gemma did, which is let's not just train on tokens, let's literally train on minimizing the KL divergence with the distribution of Gemma 27B, right?"}, {"block_idx": 99, "token_num": 6, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "So knowledge distillation there."}, {"block_idx": 100, "token_num": 32, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And you're spending the compute of literally training this 27 billion parameter model on all these tokens, just to get out this, I don't know, smaller model."}, {"block_idx": 101, "token_num": 15, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "And the distillation gives you just a faster model, smaller means faster."}, {"block_idx": 102, "token_num": 2, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Yeah."}, {"block_idx": 103, "token_num": 21, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Distillation in theory is, I think, getting out more signal from the data that you're training on."}, {"block_idx": 104, "token_num": 21, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And it's perhaps another way of getting over, not completely over, but partially helping with the data wall."}, {"block_idx": 105, "token_num": 33, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Where you only have so much data to train on, let's train this really, really big model on all these tokens and we'll distill it into this smaller one."}, {"block_idx": 106, "token_num": 23, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And maybe we can get more signal per token for this much smaller model than we would've originally if we trained it."}, {"block_idx": 107, "token_num": 15, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So if I gave you $10 trillion, how would you spend it?"}, {"block_idx": 108, "token_num": 10, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "I mean you can't buy an island or whatever."}, {"block_idx": 109, "token_num": 22, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "How would you allocate it in terms of improving the big model versus maybe paying for HF in the RLHF?"}, {"block_idx": 110, "token_num": 2, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Or-"}, {"block_idx": 111, "token_num": 4, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Yeah, yeah."}, {"block_idx": 112, "token_num": 31, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I think there's a lot of these secrets and details about training these large models that I just don't know, and are only privy to the large labs."}, {"block_idx": 113, "token_num": 26, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And the issue is, I would waste a lot of that money if I even attempted this, because I wouldn't know those things."}, {"block_idx": 114, "token_num": 31, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Suspending a lot of disbelief and assuming you had the know- how, or if you're saying you have to operate with the limited information you have now-"}, {"block_idx": 115, "token_num": 6, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "No, no, no."}, {"block_idx": 116, "token_num": 38, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Actually, I would say you swoop in and you get all the information, all the little heuristics, all the little parameters, all the parameters that define how the thing is trained."}, {"block_idx": 117, "token_num": 4, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Mm-hmm."}, {"block_idx": 118, "token_num": 23, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "If we look in how to invest money for the next five years in terms of maximizing what you called raw intelligence-"}, {"block_idx": 119, "token_num": 9, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I mean, isn't the answer really simple?"}, {"block_idx": 120, "token_num": 11, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "You just try to get as much compute as possible."}, {"block_idx": 121, "token_num": 16, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "At the end of the day all you need to buy, is the GPUs."}, {"block_idx": 122, "token_num": 8, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "And then the researchers can find all…"}, {"block_idx": 123, "token_num": 18, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "You can tune whether you want to pre-train a big model or a small model."}, {"block_idx": 124, "token_num": 25, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Well this gets into the question of are you really limited by compute and money, or are you limited by these other things?"}, {"block_idx": 125, "token_num": 23, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I'm more privy to Arvid's belief that we're sort of idea-limited, but there's always that like-"}, {"block_idx": 126, "token_num": 17, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "But if you have a lot of compute, you can run a lot of experiments."}, {"block_idx": 127, "token_num": 18, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So you would run a lot of experiments versus use that compute to trend a gigantic model?"}, {"block_idx": 128, "token_num": 19, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "I would, but I do believe that we are limited in terms of ideas that we have."}]}