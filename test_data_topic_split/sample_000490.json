{"sample_idx": 3, "start_block_idx": 104, "last_block_idx": 161, "block_list": [{"block_idx": 0, "token_num": 75, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "呃，whatever就是。如果我们去看这个o1 preview，我觉得最让我感兴趣的是那个那个math的那个example就你可以看出来solve一个math问题的时候我觉得math和coding其实overall还是比较比较相似的，在很多方面，但如果你看到math的问题，它可以写写写说哦，我要这样，然后他说，哦，alternative。"}, {"block_idx": 1, "token_num": 4, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Let's consider this."}, {"block_idx": 2, "token_num": 5, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Oh, actually alternative."}, {"block_idx": 3, "token_num": 5, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Let me consider this."}, {"block_idx": 4, "token_num": 112, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "我觉得他在不断的去self refine他自己的一个一个一个thinking的过程。which sounds to me is pretty fascinating这样就不需要我human in the loop去correct很多mistake了。这是就是说o1我觉得好的方面啊，不过这个o1不好的方面，就是说这个怎么来定义，这是个非常somehow mediocre graduate对吧啊，我觉得你应该在网上看到很多这个网友就是拿这个o1问了一个非常有意思的问题，就是说how to install这个什么CUDA还是啥来着？我忘了。"}, {"block_idx": 5, "token_num": 66, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "然后这个那个网友晒出的东西就是说这个东西think for twenty-seven hours想告诉你说I don't know它这个训练数据非常focus on的方面，它的表现还是非常惊惊艳的，但在另一些方面，其实它还有很多的局限，我非常期待他们未来的工作可以去further address。"}, {"block_idx": 6, "token_num": 17, "speaker_id": "speaker_1", "speaker_name": "Monica", "text": "你觉得还有哪一些局限是希望在可能下一个版本里面看到？"}, {"block_idx": 7, "token_num": 22, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "呃，我觉得就是说这个几个方面吧，就是，呃，怎么样让它的数据的coverage更多。"}, {"block_idx": 8, "token_num": 47, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "怎么让他的数据的evaluation的的方式可以更scalable，呃，他有一篇我觉得OpenAI的这个工作让我非常fascinating就很多年前的一个叫一个叫PRM的工作叫par。"}, {"block_idx": 9, "token_num": 27, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Process Reward Model Reward agenda The sequence Our reward subsequence uh, you have to open a link and pay with, uh, less."}, {"block_idx": 10, "token_num": 47, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "By step by step呃，我觉得open AI应该是花了非常多的时间去invest怎么来做数据？这个方面就是他们具体的工工作，我不知道了，我觉得这个对于不论是Google还是这个。"}, {"block_idx": 11, "token_num": 42, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Whatever the fundamental recipe is all about, how can you create a lot of high quality data and then it's about how you define high quality data so you show a scalable way to to filter out high quality data."}, {"block_idx": 12, "token_num": 110, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "啊，然后你你filter high quality data的时候，很多时候你给它标reward signal的时候，你又需要一个scalable way to not just give a sparse reward对吧？不是像数学问题说哦，OK，eventually it's right or wrong啊。但是对于很多的问题，其实是没有一个closed solution的，你非常难去evaluate这个东西是一个好还是坏的事情，这样子的话你怎么可以define一个systematic way to actually scale to label high quality data，我觉得这是个非常我觉得fascinating的问题。"}, {"block_idx": 13, "token_num": 29, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "但是说如果这个问题可以被解决，我可以期待这些reasoning的task可以有再往上一个质的水平的飞跃。"}, {"block_idx": 14, "token_num": 65, "speaker_id": "speaker_1", "speaker_name": "Monica", "text": "你也提到，其实OpenAI就放出了很多跟数据相关的这些工作啊。那你觉得说就是要要训练出o1这样的这个这个model你觉得需要怎样的一些？呃，跟以前我们训练LM不一样的这个数据获得和处理这些数据有有哪些难点？"}, {"block_idx": 15, "token_num": 10, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "嗯，我觉得这个就是非常好的问题啊。"}, {"block_idx": 16, "token_num": 15, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "呃，如果。如果我们take a step back OpenAI刚发布第一。"}, {"block_idx": 17, "token_num": 4, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "InstructGPT."}, {"block_idx": 18, "token_num": 99, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "的时候啊，当就是很多年前Google还非常focus on做high quality的sft的这个数据的时候。然后这个InstructGPT剑走偏锋说我要做这个preference的数据。其实fundamentally，不论你是做sft还是做呃呃RLHF的preference data你都需要非常好的数据，呃，但是这边的一个tricky的点在于的high quality的数据其实是比sft的high quality数据好做的。"}, {"block_idx": 19, "token_num": 95, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "所以说他们play的第一个track是用了一个比较smart的方式来，可以更加highly scalable high quality的数据是一个preference的数据，我觉得这是他们的第一个的这个让我觉得非常惊艳的地方，OK，然后你可以做了这个这个preference的数据了啊，但这是个sparse preference啊。sparse preference的意思就是说你只有把这个conversation结束之后，你只是说对于整个entire conversation，你觉得这是好还是坏？"}, {"block_idx": 20, "token_num": 36, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "啊，但是这个的话就是说如果中间有很多intermediate step reasoning，你其实没有办法，就是说对其实中间的每一个intermediate step来做打分，然后。"}, {"block_idx": 21, "token_num": 7, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "OK, let's continue our work."}, {"block_idx": 22, "token_num": 23, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "What else can we do to actually creating this preference data, but also preference data with the fine-grained work?"}, {"block_idx": 23, "token_num": 6, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Let's verify step by step."}, {"block_idx": 24, "token_num": 14, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Verify the preference data, not just by the final rating, right?"}, {"block_idx": 25, "token_num": 10, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "How can I actually verify for the intermediate step?"}, {"block_idx": 26, "token_num": 6, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Let's verify step by step."}, {"block_idx": 27, "token_num": 87, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "这样的一篇那个他们其实发了一个数据集叫prm 800K就是一个verify的step by step的数据集。然后我觉得其实这一套研究的思路就被他们一脉相承。到了今天啊，来做这个这个这个o1的这个过程，但是fundamentally，我觉得我们要解决的方式是怎么用一个scalable的方式来标注一些high quality的数据。"}, {"block_idx": 28, "token_num": 89, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "但是说这些high-quality的数据不一定要是一个SFT的数据，这些high-quality的数据可以是个preference的数据，或者说有可能某一天我们有比标preference数据更容易标出来。high-quality的数据可以让这个scaling law再做一个10X或100X的这个在数据方面的这个scaling law，那我觉得这个模型有可能又可以达到一个新的知识方面的飞跃。"}, {"block_idx": 29, "token_num": 22, "speaker_id": "speaker_6", "speaker_name": "Cage", "text": "刚刚Kimi提到scalable，我想讨论一下就是当时那个InstructGPT出来的时候。"}, {"block_idx": 30, "token_num": 9, "speaker_id": "speaker_6", "speaker_name": "Cage", "text": "Anthropic paper just do a Constitutional AI."}, {"block_idx": 31, "token_num": 17, "speaker_id": "speaker_6", "speaker_name": "Cage", "text": "I'm just, you know, RL from AI feedback, you know, from the."}, {"block_idx": 32, "token_num": 54, "speaker_id": "speaker_6", "speaker_name": "Cage", "text": "的话，呃，我们比如说要有高质量的reasoning tokens这样整一个数据，呃，如果我们今天去付现O问有多少会是人类的高质量标注，然后有多少未来能借助AI慢慢给做好？"}, {"block_idx": 33, "token_num": 55, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "我觉得是这样的，就是人类标注其实可以用不同的方式来使用。呃，最straight forward的方式是，呃。direct preference optimization对吧？就大家在做RLHF的时候说啊，这个training reward model太复杂了。"}, {"block_idx": 34, "token_num": 39, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "然后我在这个这个最后train RLHF的时候我要用这个PPO，对吧？我不但要有现在的模型在我的在我的memory里面，我之前的模型在里面。"}, {"block_idx": 35, "token_num": 3, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Too complicated."}, {"block_idx": 36, "token_num": 8, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Let's just train DPO, right?"}, {"block_idx": 37, "token_num": 7, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Let's just do direct preference optimization."}, {"block_idx": 38, "token_num": 76, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "大家大家做DPO的时候，这个好处在于其实我不需要这个这个呃呃这个这个机器的数据对吧？如果我人标了一些一些数据，我直接人标的数据是可以直接用来做train，我觉得这是最直接的一种一种用法。第2种的用法就是说如果你需要用RLAIF来。"}, {"block_idx": 39, "token_num": 16, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Like in like beyond the preference should be none should come from where, right?"}, {"block_idx": 40, "token_num": 7, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "It's actually chicken and egg problem."}, {"block_idx": 41, "token_num": 20, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "You want a model can you know, do good work to to help you create high quality data."}, {"block_idx": 42, "token_num": 16, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "But before that, you actually need to train a high quality model, right?"}, {"block_idx": 43, "token_num": 9, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "So just you got chicken and egg problem."}, {"block_idx": 44, "token_num": 119, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "所以大家会做的事情说，OK，我先用人来标一些数据，然后我把人标的数据来train一个reward model，那我有了这样的一个reward model之后，就是说有别的数据，它其实没有preference，我们可以用人的方式来像人一样来标它其实上，然后这样的标的方式其实相当于说是一个RLAIF的方式来给这个模型preference feedback啊，但是这个RLAIF就有可能，呃，又有它的potential的问题，就会导致这个东西叫reward hacking对吧？就是说OK，就是这个。"}, {"block_idx": 45, "token_num": 45, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "作为一个人而言，他给我的不一样的这个，呃，response，我有可能非常systematically去analyze就说，哦，okay，我知道这是好，这是不好，但比如说你现在train了一。"}, {"block_idx": 46, "token_num": 9, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "If you don't care about safety, right?"}, {"block_idx": 47, "token_num": 15, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "So if I'm asking a unsafe question, the model was just not responsive."}, {"block_idx": 48, "token_num": 32, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "And then from a reward model perspective, you know, I might just say, OK, if you do not respond to me, that's just a good thing."}, {"block_idx": 49, "token_num": 3, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "Thank you."}, {"block_idx": 50, "token_num": 9, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "This is a really bad scenario, right?"}, {"block_idx": 51, "token_num": 13, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "It should response you, but the language model might just explode."}, {"block_idx": 52, "token_num": 13, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "This, this, this, this back door of the model."}, {"block_idx": 53, "token_num": 14, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "So overall it's a very, it's a very interesting and tricky topic."}, {"block_idx": 54, "token_num": 15, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "We need to spend more time investing on how to train a reward model."}, {"block_idx": 55, "token_num": 22, "speaker_id": "speaker_4", "speaker_name": "Kimi Kong", "text": "I think that's actually a fundamental component on how can we further scale our HF training or our AIF training."}, {"block_idx": 56, "token_num": 111, "speaker_id": "speaker_5", "speaker_name": "苏辉", "text": "呃，关于我使用o1的一些例子，因为我之前很喜欢除了leetcode的这种周赛的题去贴之外，我会测一个那种复杂场景下的啊，就是旅游问题，就是我所我所谓的复杂场景，就是指你很可能是一个家庭，然后呢，你还要去进行一些跨国的旅行，然后你可以贴一些，然后我的prompt里面一般会贴一些这个你买的机票的时间啊，然后有一些景点，然后基本上之前在测试GPT-4的时候，它。"}, {"block_idx": 57, "token_num": 113, "speaker_id": "speaker_5", "speaker_name": "苏辉", "text": "他有的时候，呃，他会给出一个看上去还可以，但是其实你去仔细看里面的一些行动细节，你会发现他，比如说他根本没有照顾到我这段路程的时间在车程上的时间，导致我这天可能就奔波于车程。其实我在今天的时间非常少，然后呃这种细节上，我就是他并没有考虑特别好，但我其实我这次又测了一遍哇，然后我觉得其实让我非常的impressive，因为。"}]}