{"sample_idx": 30, "start_block_idx": 3033, "last_block_idx": 3134, "block_list": [{"block_idx": 0, "token_num": 19, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "I'm lucky to work with a bunch of great engineers because I am definitely not a great engineer."}, {"block_idx": 1, "token_num": 5, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "And the infrastructure especially."}, {"block_idx": 2, "token_num": 5, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Yeah, for sure."}, {"block_idx": 3, "token_num": 10, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "So it turns out TLDR, it worked."}, {"block_idx": 4, "token_num": 3, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "It worked."}, {"block_idx": 5, "token_num": 2, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Yeah."}, {"block_idx": 6, "token_num": 22, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And I think this is important because you could have imagined a world where you set after towards monospecificity."}, {"block_idx": 7, "token_num": 6, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Chris, this is great."}, {"block_idx": 8, "token_num": 20, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "It works on a one-layer model, but one-layer models are really idiosyncratic."}, {"block_idx": 9, "token_num": 36, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Maybe that's just something, maybe the linear representation hypothesis and superposition hypothesis is the right way to understand a one-layer model, but it's not the right way to understand larger models."}, {"block_idx": 10, "token_num": 34, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So I think, I mean, first of all, the Cunningham and all paper sort of cut through that a little bit and sort of suggested that this wasn't the case."}, {"block_idx": 11, "token_num": 42, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "But Scaling Monospecificity sort of I think was significant evidence that even for very large models, and we did it on Claude 3 Sonnet, which at that point was one of our production models."}, {"block_idx": 12, "token_num": 15, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Even these models seemed to be substantially explained, at least by linear features."}, {"block_idx": 13, "token_num": 17, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And doing dictionary learning on them works, and as you learn more features, you go"}, {"block_idx": 14, "token_num": 7, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "and you explain more and more."}, {"block_idx": 15, "token_num": 11, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So that's, I think, quite a promising sign."}, {"block_idx": 16, "token_num": 17, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And you find now really fascinating abstract features, and the features are also multimodal."}, {"block_idx": 17, "token_num": 15, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "They respond to images and texts for the same concept, which is fun."}, {"block_idx": 18, "token_num": 2, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Yeah."}, {"block_idx": 19, "token_num": 5, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Can you explain that?"}, {"block_idx": 20, "token_num": 16, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "I mean, backdoor, there's just a lot of examples that you can-"}, {"block_idx": 21, "token_num": 2, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Yeah."}, {"block_idx": 22, "token_num": 7, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So maybe let's start with that."}, {"block_idx": 23, "token_num": 20, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "One example to start, which is we found some features around security vulnerabilities and backdooring code."}, {"block_idx": 24, "token_num": 10, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So turns out those are actually two different features."}, {"block_idx": 25, "token_num": 31, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So there's a security vulnerability feature, and if you force it active, Claude it will start to go and write security vulnerabilities like buffer overflows into code."}, {"block_idx": 26, "token_num": 37, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And also fires for all kinds of things, some of the top data set examples where things like dash dash, disable SSL or something like this, which are sort of obviously really insecure."}, {"block_idx": 27, "token_num": 27, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "So at this point, maybe it's just because the examples are presented that way, it's kind of surface a little bit more obvious examples."}, {"block_idx": 28, "token_num": 27, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "I guess the idea is that down the line it might be able to detect more nuance like deception or bugs or that kind of stuff."}, {"block_idx": 29, "token_num": 2, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Yeah."}, {"block_idx": 30, "token_num": 10, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Well, maybe I want to distinguish two things."}, {"block_idx": 31, "token_num": 14, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So one is the complexity of the feature or the concept, right?"}, {"block_idx": 32, "token_num": 17, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And the other is the nuance of how subtle the examples we're looking at, right?."}, {"block_idx": 33, "token_num": 23, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So when we show the top data set examples, those are the most extreme examples that cause that feature to activate."}, {"block_idx": 34, "token_num": 14, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And so it doesn't mean that it doesn't fire for more subtle things."}, {"block_idx": 35, "token_num": 40, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So that insecure code feature, the stuff that it fires most strongly for are these really obvious disable the security type things, but it also fires for buffer overflows and more subtle security vulnerabilities in code."}, {"block_idx": 36, "token_num": 7, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "These features are all multimodal."}, {"block_idx": 37, "token_num": 13, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "You could ask it like, “What images activate this feature?”"}, {"block_idx": 38, "token_num": 34, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And it turns out that the security vulnerability feature activates for images of people clicking on Chrome to go past this website, the SSL certificate might be wrong or something like this."}, {"block_idx": 39, "token_num": 38, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Another thing that's very entertaining is there's backdoors in code feature, like you activate it, it goes and Claude writes a backdoor that will go and dump your data to port or something."}, {"block_idx": 40, "token_num": 16, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "But you can ask, “Okay, what images activate the backdoor feature?”"}, {"block_idx": 41, "token_num": 9, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "It was devices with hidden cameras in them."}, {"block_idx": 42, "token_num": 33, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So there's a whole apparently genre of people going and selling devices that look innocuous that have hidden cameras, and they have ads that has this hidden camera in it?"}, {"block_idx": 43, "token_num": 13, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And I guess that is the physical version of a backdoor."}, {"block_idx": 44, "token_num": 61, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And so it sort of shows you how abstract these concepts are, and I just thought that was… I'm sort of sad that there's a whole market of people selling devices like that, but I was kind of delighted that that was the thing that it came up with as the top image examples for the feature."}, {"block_idx": 45, "token_num": 5, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Yeah, it's nice."}, {"block_idx": 46, "token_num": 4, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "It's multimodal."}, {"block_idx": 47, "token_num": 5, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "It's multi almost context."}, {"block_idx": 48, "token_num": 10, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "It's broad, strong definition of a singular concept."}, {"block_idx": 49, "token_num": 3, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "It's nice."}, {"block_idx": 50, "token_num": 2, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Yeah."}, {"block_idx": 51, "token_num": 20, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "To me, one of the really interesting features, especially for AI safety, is deception and lying."}, {"block_idx": 52, "token_num": 23, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "And the possibility that these kinds of methods could detect lying in a model, especially get smarter and smarter and smarter."}, {"block_idx": 53, "token_num": 30, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Presumably that's a big threat over super intelligent model that it can deceive the people operating it as to its intentions or any of that kind of stuff."}, {"block_idx": 54, "token_num": 11, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "So what have you learned from detecting lying inside models?"}, {"block_idx": 55, "token_num": 27, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Yeah, so I think we're in some ways in early days for that, we find quite a few features related to deception and lying."}, {"block_idx": 56, "token_num": 25, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "There's one feature where it fires for people lying and being deceptive, and you force it active and Claude starts lying to you."}, {"block_idx": 57, "token_num": 7, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So we have a deception feature."}, {"block_idx": 58, "token_num": 28, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "I mean, there's all kinds of other features about withholding information and not answering questions, features about power seeking and coups and stuff like that."}, {"block_idx": 59, "token_num": 38, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So there's a lot of features that are kind of related to spooky things, and if you force them active Claude will behave in ways that are… they're not the kinds of behaviors you want."}, {"block_idx": 60, "token_num": 16, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "What are possible next exciting directions to you in the space of Mechinterp?"}, {"block_idx": 61, "token_num": 8, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Well, there's a lot of things."}, {"block_idx": 62, "token_num": 39, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So for one thing, I would really like to get to a point where we have shortcuts where we can really understand not just the features, but then use that to understand the computation of models."}, {"block_idx": 63, "token_num": 11, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "That relief for me is the ultimate goal of this."}, {"block_idx": 64, "token_num": 13, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And there's been some work, we put out a few things."}, {"block_idx": 65, "token_num": 26, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "There's a paper from Sam Marks that does some stuff like this, and there's been, I'd say some work around the edges here."}, {"block_idx": 66, "token_num": 30, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "But I think there's a lot more to do, and I think that will be a very exciting thing that's related to a challenge we call interference weights."}, {"block_idx": 67, "token_num": 44, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Where due to superstition, if you just sort of naively look at what features are connected together, there may be some weights that don't exist in the upstairs model, but are just sort of artifacts of superstition."}, {"block_idx": 68, "token_num": 32, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So that's a technical challenge Related to that, I think another exciting direction is just you might think of sparse autoencoders as being kind of like a telescope."}, {"block_idx": 69, "token_num": 43, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "They allow us to look out and see all these features that are out there, and as we build better and better sparse autoencoders, we better and better at dictionary learning, we see more and more stars."}, {"block_idx": 70, "token_num": 10, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And we zoom in on smaller and smaller stars."}, {"block_idx": 71, "token_num": 18, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "There's a lot of evidence that we're only still seeing a very small fraction of the stars."}, {"block_idx": 72, "token_num": 16, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "There's a lot of matter in our neural network universe that we can't observe yet."}, {"block_idx": 73, "token_num": 35, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And it may be that we'll never be able to have fine enough instruments to observe it, and maybe some of it just isn't possible, isn't computationally tractable to observe."}, {"block_idx": 74, "token_num": 32, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "So it's sort of a kind of dark matter in not in maybe the sense of modern astronomy of early astronomy when we didn't know what this unexplained matter is."}, {"block_idx": 75, "token_num": 41, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And so I think a lot about that dark matter and whether we'll ever observe it and what that means for safety if we can't observe it, if some significant fraction of neural networks are not accessible to us."}, {"block_idx": 76, "token_num": 27, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Another question that I think a lot about is at the end of the day, mechanistic interpolation is this very microscopic approach to interpolation."}, {"block_idx": 77, "token_num": 27, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "It's trying to understand things in a very fine-grained way, but a lot of the questions we care about are very macroscopic."}, {"block_idx": 78, "token_num": 22, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "We care about these questions about neural network behavior, and I think that's the thing that I care most about."}, {"block_idx": 79, "token_num": 15, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "But there's lots of other sort of larger-scale questions you might care about."}, {"block_idx": 80, "token_num": 21, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And the nice thing about having a very microscopic approach is it's maybe easier to ask, is this true?"}, {"block_idx": 81, "token_num": 14, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "But the downside is its much further from the things we care about."}, {"block_idx": 82, "token_num": 10, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And so we now have this ladder to climb."}, {"block_idx": 83, "token_num": 39, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And I think there's a question of will we be able to find, are there larger-scale abstractions that we can use to understand neural networks that can we get up from this very microscopic approach?"}, {"block_idx": 84, "token_num": 2, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "Yeah."}, {"block_idx": 85, "token_num": 10, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "You've written about this as kind of organs question."}, {"block_idx": 86, "token_num": 4, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Yeah, exactly."}, {"block_idx": 87, "token_num": 38, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "If we think of interpretability as a kind of anatomy of neural networks, most of the circus threads involve studying tiny little veins looking at the small scale and individual neurons and how they connect."}, {"block_idx": 88, "token_num": 15, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "However, there are many natural questions that the small-scale approach doesn't address."}, {"block_idx": 89, "token_num": 31, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "In contrast, the most prominent abstractions and biological anatomy involve larger-scale structures like individual organs, like the heart or entire organ systems like the respiratory system."}, {"block_idx": 90, "token_num": 21, "speaker_id": "speaker_1", "speaker_name": "Lex Fridman", "text": "And so we wonder, is there a respiratory system or heart or brain region of an artificial neural network?"}, {"block_idx": 91, "token_num": 4, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "Yeah, exactly."}, {"block_idx": 92, "token_num": 12, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And I mean, if you think about science, right?"}, {"block_idx": 93, "token_num": 13, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "A lot of scientific fields investigate things at many level of abstraction."}, {"block_idx": 94, "token_num": 49, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "In biology, you have molecular biology studying proteins and molecules and so on, and they have cellular biology, and then you have histology studying tissues, and then you have anatomy, and then you have zoology, and then you have ecology."}, {"block_idx": 95, "token_num": 35, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And so you have many, many levels of abstraction or physics, maybe you have a physics of individual particles, and then statistical physics gives you thermodynamics and things like this."}, {"block_idx": 96, "token_num": 10, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And so you often have different levels of abstraction."}, {"block_idx": 97, "token_num": 36, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And I think that right now we have mechanistic interpretability, if it succeeds, is sort of like a microbiology of neural networks, but we want something more like anatomy."}, {"block_idx": 98, "token_num": 17, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And a question you might ask is, “Why can't you just go there directly?”"}, {"block_idx": 99, "token_num": 15, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "And I think the answer is superstition, at least in significant part."}, {"block_idx": 100, "token_num": 33, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "It's that it's actually very hard to see this macroscopic structure without first sort of breaking down the microscopic structure in the right way and then studying how it connects together."}, {"block_idx": 101, "token_num": 33, "speaker_id": "speaker_3", "speaker_name": "Chris Olah", "text": "But I'm hopeful that there is going to be something much larger than features and circuits and that we're going to be able to have a story that involves much bigger things."}]}