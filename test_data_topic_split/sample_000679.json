{"sample_idx": 20, "start_block_idx": 1853, "last_block_idx": 1958, "block_list": [{"block_idx": 0, "token_num": 30, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "OK, so actually the thing I was literally just working on yesterday has to do with biological evolution and the question of different fitness functions in biological evolution."}, {"block_idx": 1, "token_num": 13, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "OK, so I've got these things that they're making little patterns."}, {"block_idx": 2, "token_num": 14, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "One fitness function could be make the pattern be as tall as possible."}, {"block_idx": 3, "token_num": 28, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Another fitness function, make it be as wide as possible, make it be as close to aspect ratio Ï€ as possible, and so on."}, {"block_idx": 4, "token_num": 26, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Right now I can see because I've got these whole, you know, I've got these whole pictures of all possible paths of evolution."}, {"block_idx": 5, "token_num": 18, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "So for some simple cases, there might be, you know, a billion different possibilities."}, {"block_idx": 6, "token_num": 24, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "But like I've mapped out all the paths, different fitness functions lead to different ways of exploring that space of possible paths."}, {"block_idx": 7, "token_num": 21, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Well, yeah, you're, you're trying to do different things depending on the different on the fitness function."}, {"block_idx": 8, "token_num": 2, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Exactly."}, {"block_idx": 9, "token_num": 17, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "So now the question is what are reasonable fitness functions and what consequences will they have?"}, {"block_idx": 10, "token_num": 4, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "There are fitness."}, {"block_idx": 11, "token_num": 5, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Functions reasonable to who?"}, {"block_idx": 12, "token_num": 4, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "That's my question."}, {"block_idx": 13, "token_num": 10, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "I mean, that's that, that's the point."}, {"block_idx": 14, "token_num": 18, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "So there are ones that are that are OK, so, so let's give some examples."}, {"block_idx": 15, "token_num": 17, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "So there's a fitness function that in this kind of space is kind of fairly smooth."}, {"block_idx": 16, "token_num": 19, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "There's a fitness function that says, I want this particular image, I want this particular pattern."}, {"block_idx": 17, "token_num": 22, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "That fitness function is a very different kind of fitness function that has different levels of evolvability to it."}, {"block_idx": 18, "token_num": 30, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "In other words, the fitness function that says more or less, I'm going to build a wall and I want it to be 6 feet high."}, {"block_idx": 19, "token_num": 19, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "That's that's a fitness function that allows many shapes of rocks to be used to make that wall."}, {"block_idx": 20, "token_num": 28, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "The fitness function that says I want this particular wall with this particular micro detailing at the top is a much more difficult to satisfy fitness function."}, {"block_idx": 21, "token_num": 6, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Sure, like you can."}, {"block_idx": 22, "token_num": 19, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "You can have narrower targets and then you need a more powerful planner to hit the narrower target."}, {"block_idx": 23, "token_num": 2, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Right."}, {"block_idx": 24, "token_num": 14, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "But but I think what is tricky and the same thing comes up."}, {"block_idx": 25, "token_num": 28, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "I mean, this is a, you know, this is related to the, this whole observer theory thing that I studied and so on."}, {"block_idx": 26, "token_num": 18, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "It's also related to what happens in drama when you're looking at reference frames and so on."}, {"block_idx": 27, "token_num": 28, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "You're, you're defining what's a, what's kind of a, a, a reasonable way, what's a reasonable, what is a reasonable goal."}, {"block_idx": 28, "token_num": 23, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Basically, you can talk about it in terms of goals, You can talk about it in terms of fitness functions."}, {"block_idx": 29, "token_num": 31, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "So if you mean like attainable for the search process, then for example, a freely rotating wheel is an extremely difficult goal for a natural selection to hit."}, {"block_idx": 30, "token_num": 23, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Wikipedia lists 3 known cases where freely rotating wheels evolved, and it's ATP synthase the bacterial flagellum."}, {"block_idx": 31, "token_num": 12, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "And like one other macro piece of macro anatomy I forget."}, {"block_idx": 32, "token_num": 10, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "And it's not that wheels aren't useful in biology."}, {"block_idx": 33, "token_num": 33, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "ATP synthase, the bacterial flagellum, There's, you know, these things are enormously useful, but it's just very hard to evolve a freely rotating wheel."}, {"block_idx": 34, "token_num": 25, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "And, and two of the cases we know are cases where it's just like these particular molecules that happen to behave like wheels."}, {"block_idx": 35, "token_num": 43, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "If it's gross anatomy, then how then it's, you know, very difficult to gradually, incrementally on a path that gets rewarded on each intermediate step to find the anatomy that can gradually develop into a rotating wheel."}, {"block_idx": 36, "token_num": 7, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "It's much easier to have eyes."}, {"block_idx": 37, "token_num": 6, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Lots of things have eyes."}, {"block_idx": 38, "token_num": 2, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Right."}, {"block_idx": 39, "token_num": 38, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "But but so the point that I'm getting to is you have made the statement that in the space of all possible goals, most of them don't have, you know, we'll crush out humans."}, {"block_idx": 40, "token_num": 6, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "That's basically what you're saying."}, {"block_idx": 41, "token_num": 21, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "If we look at the space of all possible goals, most of them don't have a place for humans."}, {"block_idx": 42, "token_num": 20, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "So what I'm trying to dig into is what do we mean by the space of all possible goals?"}, {"block_idx": 43, "token_num": 53, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "In other words, if we, if we allow the goal to be, you know, this particular arrangement of atoms is achieved right, then that's then you know, again, I'm, I'm, I'm claiming it's not so obvious what the spaceable possible goals means."}, {"block_idx": 44, "token_num": 2, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "OK."}, {"block_idx": 45, "token_num": 29, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "I agree that if we want to dive into the subtleties, then there are all kinds of subtleties we can start listing out."}, {"block_idx": 46, "token_num": 30, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "And the thing I would point out is that sometimes there's a lot of subtleties, but they, you know, don't end up filtering reality."}, {"block_idx": 47, "token_num": 14, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "The subtleties don't filter reality to give you what you want."}, {"block_idx": 48, "token_num": 8, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Is is where we're going to be."}, {"block_idx": 49, "token_num": 95, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Going I, I I agree, but I mean, I think the, the, the statement you're making is that, OK, so one, one statement you might be making, but I think you would view that as a science fiction statement that you're not making is that goals that humans impose on the AIS, like make killer drones that go and take as much territory as possible or whatever else that those kinds of goals might have as a consequence, the crushing out of humans."}, {"block_idx": 50, "token_num": 2, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Correct."}, {"block_idx": 51, "token_num": 4, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "I suppose to."}, {"block_idx": 52, "token_num": 21, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "I think they just don't have the power to determine what the Super intelligent version of the AI wants exactly."}, {"block_idx": 53, "token_num": 53, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "No, but but what you're saying is that in this, you know, when we're talking about what the innards of the AI are going to do, then we're in the space of all possible goals and we're out of a place where we know what's going on."}, {"block_idx": 54, "token_num": 41, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Whereas if we say the goal is, you know, take as much territory with killer drones as you can, type thing or whatever else, then we already know what those kinds of goals look like."}, {"block_idx": 55, "token_num": 22, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "And we also know those are goals which have the feature that will crush out the humans, so to speak."}, {"block_idx": 56, "token_num": 39, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Yeah, I think that's kind of like a hopeful fairy tale version of it where the punishment that is brought down upon humanity stems from humanity's obvious lack of wisdom that the author knew better than."}, {"block_idx": 57, "token_num": 11, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "But you know, the protagonist didn't know better than."}, {"block_idx": 58, "token_num": 13, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "And people are doing, people are actually doing things that stupid."}, {"block_idx": 59, "token_num": 4, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "And it's disturbing."}, {"block_idx": 60, "token_num": 10, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "But even if they weren't that stupid, but."}, {"block_idx": 61, "token_num": 15, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "You are making fail here what is what is essentially a mathematical formal statement."}, {"block_idx": 62, "token_num": 66, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "You're saying, you know, absent those sort of easily understandable and arguably, you know, destructive stupid goals type thing that even absent that the goals that are intrinsic goals inside the AI that come from the fact that there are features of how the AI works which are not determined by us, which are just, well, they're the."}, {"block_idx": 63, "token_num": 2, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Features."}, {"block_idx": 64, "token_num": 24, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "They were determined by the training program we created, but we were not in deliberate control of them because it wasn't predictable."}, {"block_idx": 65, "token_num": 12, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "It's not that magic happened, it's that we didn't understand."}, {"block_idx": 66, "token_num": 22, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Right, there's some computational irreducibility story that leads to lots of unexpected things that we cannot readily predict."}, {"block_idx": 67, "token_num": 14, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "It could be computationally reducible, we just don't understand it."}, {"block_idx": 68, "token_num": 3, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Fair enough."}, {"block_idx": 69, "token_num": 24, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "But but we could understand in that case, whereas if it's irreducible, which is I think the real case."}, {"block_idx": 70, "token_num": 6, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "And if it was we?"}, {"block_idx": 71, "token_num": 5, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Don't even have a."}, {"block_idx": 72, "token_num": 2, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Hope."}, {"block_idx": 73, "token_num": 32, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "And if it was irreducible then and we But we could just like you know, grind through a trillion operations to figure it out, we'd be fine."}, {"block_idx": 74, "token_num": 5, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Yeah, fair enough."}, {"block_idx": 75, "token_num": 13, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "But but the bottom line is it's done something that we can't."}, {"block_idx": 76, "token_num": 15, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "We didn't readily set up, we didn't imagine this is where it's going."}, {"block_idx": 77, "token_num": 20, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "We didn't say, you know, you are going to, you know, it's the death wish."}, {"block_idx": 78, "token_num": 19, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "We're going to wipe out the humans so that we preserve the mountain lions or whatever it is."}, {"block_idx": 79, "token_num": 9, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "It it's, you know, it's a."}, {"block_idx": 80, "token_num": 27, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "So it is because of that in a sense unpredictable, almost random internal kind of generation of goals as you would describe generation of goals."}, {"block_idx": 81, "token_num": 19, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "And you're saying that in the space of all possible such goals that you might make several statements."}, {"block_idx": 82, "token_num": 34, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "You might say, as we sample over many of those goals, we will eventually hit the jackpot, so to speak, and hit the goal that kills all the humans."}, {"block_idx": 83, "token_num": 6, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "No, no, no."}, {"block_idx": 84, "token_num": 5, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Or you might say."}, {"block_idx": 85, "token_num": 15, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "The the, the jackpot is the goal that doesn't kill all the humans."}, {"block_idx": 86, "token_num": 9, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "That's the that's the one that's hard to."}, {"block_idx": 87, "token_num": 21, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Depends on what your version of of what your what your, what your meta goal is so to speak."}, {"block_idx": 88, "token_num": 17, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "But yes I understand your and my meta goal is to not kill all the humans."}, {"block_idx": 89, "token_num": 21, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Yeah, so but but in in calling it the jackpot, I was talking about how rare is it?"}, {"block_idx": 90, "token_num": 34, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "And I think that the thing that doesn't kill everyone is rare and you don't need to look for and you need to look 0 hard to find something that kills everyone."}, {"block_idx": 91, "token_num": 12, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "OK, you don't have to look for it at all."}, {"block_idx": 92, "token_num": 35, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "OK, so your assertion is that these, I mean, it's one thing to have human defined goals like make money, you know, take territory, whatever else, right?"}, {"block_idx": 93, "token_num": 20, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "It's another thing to have these incomprehensible to us humans goals that are somehow inside the AI."}, {"block_idx": 94, "token_num": 14, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "And you it's not the incomprehensibility that that scares me."}, {"block_idx": 95, "token_num": 11, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Like maybe it turns out that there's one particular cut."}, {"block_idx": 96, "token_num": 7, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Maybe, maybe it's making diamonds."}, {"block_idx": 97, "token_num": 15, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "I can understand making diamonds, but that kills everyone as a side effect."}, {"block_idx": 98, "token_num": 12, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "So it's not the incomprehensibility that scares me."}, {"block_idx": 99, "token_num": 21, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "I I understand it's not the incomprehensibility, it's the fact that you didn't determine those goals."}, {"block_idx": 100, "token_num": 18, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Those those goals are things that were emergent goals basically inside the AI and you say."}, {"block_idx": 101, "token_num": 16, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "That that nobody to control them and then they ended up in a scary place."}, {"block_idx": 102, "token_num": 24, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "Like if aliens want to come by and give us a nice AI, then this seems a bit unnerving but fine."}, {"block_idx": 103, "token_num": 15, "speaker_id": "speaker_1", "speaker_name": "Eliezer Yudkowsky", "text": "It's not that I want to be in controls that I want to live."}, {"block_idx": 104, "token_num": 4, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "Yeah, right."}, {"block_idx": 105, "token_num": 39, "speaker_id": "speaker_2", "speaker_name": "Stephen Wolfram", "text": "So, so the point is that there are pieces inside the AI that create sort of that create things that appeared that are like goals for the AI that were not things that anybody had control over."}]}