{"sample_idx": 0, "start_block_idx": 0, "last_block_idx": 31, "block_list": [{"block_idx": 0, "token_num": 90, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "好，我看已经有观众进入到我们的直播间了啊。欢迎大家又来到我们的今天晚上的今夜科技谈呃，极客公园的今夜科技谈经常会时不时的我们就抓一抓和科技行业的热点和新的变化，然后我们约到非常优秀的嘉宾，专业的嘉宾，咱们一起来聊一聊，把变化看懂，而且还要。"}, {"block_idx": 1, "token_num": 91, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "啊，非常快速的，比别人更早的去拿到这些认知。这可能就是今夜科技谈希望能够陪大家在晚上还要聊一聊的主要的原因和目标。那今天呢，我们又要聊一聊AI了，应该说在过去这两年里边，大家都被AI呃弄得内心还是很火热的，不管是创业者还是可能我们看到的这种大厂。"}, {"block_idx": 2, "token_num": 41, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "早期的创业公司，全球可能对于AI都是一波非常强烈的热潮啊。过去一年多呢，可能我们在国内也看到了非常多的优秀的创业公司。"}, {"block_idx": 3, "token_num": 124, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "呃，大家肯定还记得哈，在这个今年呢，我们也邀请了这个创新工场的董事长，同时也是零一万物的CEO啊，开复老师来到我们的今夜科技谈的直播间，曾经聊过AI创业啊，我们当时还开玩笑说，哎呀，这可能是这一波创业里年岁最大的创业者啊。但开复老师其实浑身还是充满着活力，并且呢，在这波浪潮里边还是创造了很多让人印象深刻的这种瞬间啊。今天其实我们。"}, {"block_idx": 4, "token_num": 96, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "又要请回开复老师跟我们一起聊一聊啊。这个为什么要聊一聊呢？因为我们看到在过去一段时间里面，AI还是又发生了一些新的变化啊，包括大家可能都关注到，呃，我们也之前聊过的这个呃o1的发布其实带来了在大模型技术发展上的一个新的路线，那这个路线呢，其实对原来的这个scaling law还是带来了一个升级的。"}, {"block_idx": 5, "token_num": 113, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "版本这个所谓的升级版本就是到底scaling什么啊？出现了变化，强化学习，在预训练的基础之上又带来了一些新的可能性。那与此同时呢？这件事儿我们当时可能聊聊技术，聊聊这种技术未来的可能成长的高度。但今天可能技术的连锁反应也应该去聊一聊，尤其是要在这个时代里面用AI做事儿啊，这种AI领域的创业者更可能要思考这个问题啊，这种大的技术方向的变化。"}, {"block_idx": 6, "token_num": 98, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "大厂们，他们的动作，他们的，呃，技术战的变化以及整个行业的变化，最终会让创业者们在这个时代要做出什么样的取舍。所以今天我们要聊一聊AI在变，但是创业者要如何取舍，那我们接下来还是要请出啊，我们创新工场的董事长，同时也是零一万物的CEO，李开复老师啊，看看开复老师是不是已经连线进来了。"}, {"block_idx": 7, "token_num": 19, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "啊，感谢了哈喽，嗨，张鹏，你好，大家好。"}, {"block_idx": 8, "token_num": 118, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "啊，看开复老师一直还都是充满活力啊。今天这个呃，我们又把开复老师请来，还是非常高兴的啊。因为我想先从一个有意思的事儿，也是让我觉得挺好奇的事儿开始聊起啊。这个因为前前几天我记得看这个黄仁勋啊，专门这个发推特啊，这个说哎呀马斯克很厉害啊，这个呃，十几天就组了个十万卡的H100的集群。"}, {"block_idx": 9, "token_num": 132, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "啊，当时我们看起来说，哇，十万卡的集群H100对吧？这个觉得真有钱，然后还真的速度很快，那当然他我觉得他说这一点的时候，核心还是让我顺着我们原来的理解，就是要把这个大模型的智能提升呢，可能需要非常庞大的算力对吧？这种工程化的难度本身也很高，十万卡就是非常高的工程，难度很快实现，那但我为什么提这件事呢？是因为最近看到零一万物发了一个新的版本的模型，好像你们这个模型不是"}, {"block_idx": 10, "token_num": 44, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "就是在里边，有些的东西还是明显的比这个马斯克他们的那个模型是要好的。我就很好奇，因为我相信开复老师一定不是偷偷搞了个十万卡的。"}, {"block_idx": 11, "token_num": 5, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "集群对吧？"}, {"block_idx": 12, "token_num": 90, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "所以这个事我就很好奇对吧？这个有点违背我们原来的理解说，嗯，越大的模型，越强的模型啊，越大的模型才能越强，越强的算力才能越强。但看起来你们又不可能做到，呃，用那么大的这个算力去堆。所以这背后一定有一些有意思的值得分享的洞察，我们就先拿这个。"}, {"block_idx": 13, "token_num": 12, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "这话题了解啊怎么做到的这么神奇？"}, {"block_idx": 14, "token_num": 41, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "嗯，好的，好的，谢谢啊，呃，对，确实我们是用大约两千张H100啊，然后训练了，嗯，训练了两啊，训练了一个月。"}, {"block_idx": 15, "token_num": 4, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "两千张。"}, {"block_idx": 16, "token_num": 47, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "两千张H800，两千张H800，啊，H800，H100差不多了啊，两千张卡训练了一个月，嗯，那不过也也我们要苹果对苹果的话，呃。"}, {"block_idx": 17, "token_num": 15, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "因为马斯克的这十万张卡还没有训练出。"}, {"block_idx": 18, "token_num": 10, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "还没用过？行，对，还没有。"}, {"block_idx": 19, "token_num": 15, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "用他上一个模型应该是15000还是两万张训练。"}, {"block_idx": 20, "token_num": 102, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "嗯，所以我们大约是啊，然后可能训练了不止不止一个月，嗯，所以我们大概是用了它1/20或1/30的啊。GPU训练了跟的一个能跟它打平的模型，而且这个打平不是说自己量量自己的mmlu就算了，谁都可以自己量的啊，谁都可以刷很多题，然后把数字弄高的。我们是在一个公开的竞技场，就是伯克利大学的LM sis。"}, {"block_idx": 21, "token_num": 61, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "它是会有上千万个啊，不同的人上去啊。同时去评分，两个模型看谁高谁低，那每个模型都会被测一两万次，那一两万次之后啊。基本上用户觉得我们跟马斯克的这个模型是不分上下的。"}, {"block_idx": 22, "token_num": 16, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "嗯嗯，这能做到这样的效率，其实背后有什么可以分享的？"}, {"block_idx": 23, "token_num": 47, "speaker_id": "speaker_1", "speaker_name": "张鹏", "text": "呃，这种方法论吗？或者说我知道这一定都是你的核心机密啊，但是大概可能让我们尝试去理解，要不然我们无法想象说他为什么能够差这么多。嗯。"}, {"block_idx": 24, "token_num": 49, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "对，我觉得应该有几个因素吧，一个，是啊，我觉得我们在训练的算法上有一些非常独特的地方，嗯啊这些细节就不能不能分享了啊。然后其次我们用的是一个，呃。"}, {"block_idx": 25, "token_num": 68, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "啊mixture experts MOE的模型。嗯，那我们用的是一个啊，比特别迅速，然后呃，规模，然后总参数都要比马斯克的模型小很多，嗯啊，但是也不会小到没有办法去啊，去达到这个泛化啊。所以我们可以看到的就是，如果我们是啊。"}, {"block_idx": 26, "token_num": 81, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "是讲稠密模型的话，我们的MOE模型啊，根据我们的实验大概可以等于一个训练的很好的M200 B的模型。嗯啊，那我们的总参数可能也在200B上下，但是我们的实际的这个激活参数是远远远远小更小，嗯，所以这个的话在训练方面就有很多，呃，巧妙的tricks。"}, {"block_idx": 27, "token_num": 78, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "嗯啊，还有就是我们的啊。基础架构infrastructure做的特别好，所以我们的训练的速度也比别人要快啊。我们啊，还有可能有一些有帮助的就是数据方面吧，因为这个也没有独立出来做实验，但是我们对数据是非常用心的。嗯啊那花了很多时间收集了好的数据啊，标注还有。"}, {"block_idx": 28, "token_num": 100, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "啊，它的这个啊，Inter living，呃，顺序等等的很多很多很多的细节，嗯啊基本就是啊，数据方面是雕花雕出来的，嗯啊花了很多很多的硬功夫啊，这这这以上加起来这就是两百多个人努力了啊，五个月左右啊，得出来的呃，结果那每一个人的贡献在不同的地方，当然做预训练的团队其实很小就几个人。"}, {"block_idx": 29, "token_num": 90, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "然后再拿去做。嗯，这个后训练的人也不大，也就几个人嗯啊他们是核心的这个操作这个过程啊，然后但是背后有很多幕后的英雄，无论是我们的infrastructure，还是我们的训练框架，还是啊去很多去啊researcher去读各种paper去讨论，还有处理数据等等等等的啊，就是全公司的力量去试着把这样的一个模型价钱能够降下来。"}, {"block_idx": 30, "token_num": 75, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "因为我们看到在这个呃，大部分的这个美国的顶级的这家公司都是不惜代价的去往前冲，那中国的公司基本没有一家啊，是能够或者愿意花这样大的啊成本去做啊。GPT-4大概是一亿美金训练出来的，GPT-5应该是花了十亿美金。"}, {"block_idx": 31, "token_num": 88, "speaker_id": "speaker_2", "speaker_name": "李开复", "text": "然后嗯，GPT-6可能就是一百亿美金，然后与此类推，这样的一个做法是一种暴力出奇迹的做法。那呃，我们认为他做到今天这个状态啊，可能就是事倍还是能够有scaling law，我相信scaling law，但是可能是事倍功半的一种scaling law啊，我不会说diminishing returns，但是事倍功半。"}]}