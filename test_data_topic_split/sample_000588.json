{"sample_idx": 6, "start_block_idx": 591, "last_block_idx": 671, "block_list": [{"block_idx": 0, "token_num": 70, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "So that you can store the only the smaller thing as some sort of low-rank reduction, and the low-rank reduction, well, that… At the end of the time, when you eventually want to compute the final thing, remember that your memory band, which means that you still have some compute left that you can use for these things."}, {"block_idx": 1, "token_num": 42, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "And if you can expand the latent vector back out and somehow this is far more efficient because you're reducing… For example, maybe you're reducing vec 32 or something like the size of the vector that you're keeping."}, {"block_idx": 2, "token_num": 37, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Yeah, there's perhaps some richness in having a separate set of keys and values and query that kind of pairwise match up versus compressing that all into one in that interaction at least."}, {"block_idx": 3, "token_num": 13, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Okay, and all of that is dealing with being memory-bound."}, {"block_idx": 4, "token_num": 14, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "I mean, ultimately, how does that map to the user experience?"}, {"block_idx": 5, "token_num": 5, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Trying to get the-"}, {"block_idx": 6, "token_num": 2, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Yeah."}, {"block_idx": 7, "token_num": 26, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "The two main outcomes are: first, you can make your cache much larger because you have less space allocated for the KV cache."}, {"block_idx": 8, "token_num": 41, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "You can maybe cache a lot more aggressively in a lot more things, so you get more cache hits, which are helpful for reducing the time to first token for the reasons that were kind of described earlier."}, {"block_idx": 9, "token_num": 41, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And then the second being, when you start doing inference with more and more requests and larger and larger batch sizes, you don't see much of a slowdown as it's generating the tokens at the speed of that."}, {"block_idx": 10, "token_num": 14, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "Well, it also allows you to make your prompt bigger for certain-"}, {"block_idx": 11, "token_num": 2, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Yeah."}, {"block_idx": 12, "token_num": 28, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Yeah, so the size of your KV cache is both the size of all your prompts multiplied by the number of prompts being processed in parallel."}, {"block_idx": 13, "token_num": 10, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "So you could increase either those dimensions, right?"}, {"block_idx": 14, "token_num": 17, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "The batch size or the size of your prompts without degrading the latency of generating tokens."}, {"block_idx": 15, "token_num": 19, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Arvid, you wrote a blog post Shadow Workspace: Iterating on Code in the Background."}, {"block_idx": 16, "token_num": 15, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So what's going on [inaudible 01:04:59]?"}, {"block_idx": 17, "token_num": 28, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "So to be clear, we want there to be a lot of stuff happening in the background, and we're experimenting with a lot of things."}, {"block_idx": 18, "token_num": 30, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "Right now, we don't have much stuff happening other than the cache warming or figuring out the right context that goes into your command key prompts for example."}, {"block_idx": 19, "token_num": 40, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "But the idea is if you can actually spend computation in the background, then you can help the user maybe at a slightly longer time horizon than just predicting the next few lines that you're going to make."}, {"block_idx": 20, "token_num": 16, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "But actually in the next 10 minutes, what are you going to make?"}, {"block_idx": 21, "token_num": 15, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "And by doing it in background, you can spend more computation doing that."}, {"block_idx": 22, "token_num": 72, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "And so the idea of the Shadow Workspace that we implemented, and we use it internally for experiments is that to actually get advantage of doing stuff in the background, you want some kind of feedback signal to give back to the model because otherwise you can get higher performance by just letting the model think for longer, and so o1 is a good example of that."}, {"block_idx": 23, "token_num": 17, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "But another way you can improve performance is by letting the model iterate and get feedback."}, {"block_idx": 24, "token_num": 38, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "And so one very important piece of feedback when you're a programmer is the language server, which is this thing, it exists for most different languages, and there's a separate language server per language."}, {"block_idx": 25, "token_num": 40, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "And it can tell you, “You're using the wrong type here,” and then gives you an error, or it can allow you to go to definition and sort of understands the structure of your code."}, {"block_idx": 26, "token_num": 46, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "So language servers are extensions developed by… There is a TypeScript language server developed by the TypeScript people, a Rust language server developed by the Rust people, and then they all interface over the language server protocol to VS Code."}, {"block_idx": 27, "token_num": 27, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "So that VS Code doesn't need to have all of the different languages built into VS Code but rather you can use the existing compiler infrastructure."}, {"block_idx": 28, "token_num": 7, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "For linting purposes, what-"}, {"block_idx": 29, "token_num": 5, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "It's for linting."}, {"block_idx": 30, "token_num": 15, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "It's for going to definition and for seeing the right types that you're using."}, {"block_idx": 31, "token_num": 7, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So it's doing type checking also."}, {"block_idx": 32, "token_num": 9, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "Yes, type checking and going to references."}, {"block_idx": 33, "token_num": 17, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "And that's like when you're working in a big project, you kind of need that."}, {"block_idx": 34, "token_num": 16, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "If you don't have that, it's really hard to code in a big project."}, {"block_idx": 35, "token_num": 20, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Can you say, again, how that's being used inside Cursor, the language server protocol communication thing?"}, {"block_idx": 36, "token_num": 60, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "So it's being used in Cursor to show to the programmer just like in VS Code, but then the idea is you want to show that same information to the models, the IM models, and you want to do that in a way that doesn't affect the user because you want to do it in background."}, {"block_idx": 37, "token_num": 44, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "And so the idea behind the Shadow Workspace was, okay, one way we can do this is we spawn a separate window of Cursor that's hidden, and so you can set this flag in it and like turn it's hidden."}, {"block_idx": 38, "token_num": 11, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "There is a window but you don't actually see it."}, {"block_idx": 39, "token_num": 47, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "And inside of this window, the AI agents can modify code however they want as long as they don't save it because it's still the same folder and then can get feedback from the linters and go to definition and iterate on their code."}, {"block_idx": 40, "token_num": 18, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So literally run everything in the background as if… Right, maybe even run the code."}, {"block_idx": 41, "token_num": 11, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "So that's the eventual version and that's what you want."}, {"block_idx": 42, "token_num": 23, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "And a lot of the blog post is actually about how do you make that happen because it's a little bit tricky."}, {"block_idx": 43, "token_num": 18, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "You want it to be on the user's machine so that it exactly mirrors the user's environment."}, {"block_idx": 44, "token_num": 60, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "And then on Linux, you can do this cool thing where you can actually mirror the file system and have the AI make changes to the files, and it thinks that it's operating on the file level, but actually, that's stored in memory and you can create this kernel-like extension to make it work."}, {"block_idx": 45, "token_num": 25, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "Whereas on Mac and Windows, it's a little bit more difficult, but it's a fun technical problem, so that's why."}, {"block_idx": 46, "token_num": 18, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "One may be hacky but interesting idea that I like is holding a lock on saving."}, {"block_idx": 47, "token_num": 75, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And so basically, you can then have the language model kind of hold the lock on saving to disk and then instead of you operating in the ground truth version of the files that are saved to disk, you actually are operating what was the Shadow Workspace before and these unsaved things that only exist in memory that you still get linter errors for, and you can code in."}, {"block_idx": 48, "token_num": 56, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And then when you try to maybe run code, it's just like there's a small warning that there's a lock, and then you kind of will take back the lock from the language server if you're trying to do things concurrently or from the Shadow Workspace if you're trying to do things concurrently."}, {"block_idx": 49, "token_num": 9, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "That's such an exciting future by the way."}, {"block_idx": 50, "token_num": 20, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "It's a bit of a tangent, but to allow a model to change files, it's scary for people"}, {"block_idx": 51, "token_num": 38, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "but it's really cool, to be able to just let the agent do a set of tasks and you come back the next day and kind of observe like it's a colleague or something like that."}, {"block_idx": 52, "token_num": 49, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And I think there may be different versions of runability where, for the simple things where you're doing things in the span of a few minutes on behalf of the user as they're programming, it makes sense to make something work locally in their machine."}, {"block_idx": 53, "token_num": 64, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I think for the more aggressive things where you're making larger changes that take longer periods of time, you'll probably want to do this in some sandbox remote environment and that's another incredibly tricky problem of how do you exactly reproduce or mostly reproduce to the point of it being effectively equivalent for running code the user's environment with this remote sandbox."}, {"block_idx": 54, "token_num": 11, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "I'm curious what kind of agents you want for coding?"}, {"block_idx": 55, "token_num": 8, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "Do you want them to find bugs?"}, {"block_idx": 56, "token_num": 9, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "Do you want them to implement new features?"}, {"block_idx": 57, "token_num": 6, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "What agents do you want?"}, {"block_idx": 58, "token_num": 18, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So by the way, when I think about agents, I don't think just about coding."}, {"block_idx": 59, "token_num": 29, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "I think so for this particular podcast, there's video editing and a lot of… If you look in Adobe, a lot… There's code behind."}, {"block_idx": 60, "token_num": 57, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "It's very poorly documented code, but you can interact with Premiere, for example, using code, and basically all the uploading, everything I do on YouTube, everything as you could probably imagine, I do all of that through code and including translation and overdubbing, all of this."}, {"block_idx": 61, "token_num": 10, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So I envision all of those kinds of tasks."}, {"block_idx": 62, "token_num": 20, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "So automating many of the tasks that don't have to do directly with the editing, so that."}, {"block_idx": 63, "token_num": 9, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Okay, that's what I was thinking about."}, {"block_idx": 64, "token_num": 38, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "But in terms of coding, I would be fundamentally thinking about bug finding, many levels of kind of bug finding and also bug finding like logical bugs, not logical like spiritual bugs or something."}, {"block_idx": 65, "token_num": 13, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Ones like big directions of implementation, that kind of stuff."}, {"block_idx": 66, "token_num": 17, "speaker_id": "speaker_4", "speaker_name": "Sualeh", "text": "Magical [inaudible 01:11:39] and bug finding."}, {"block_idx": 67, "token_num": 2, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Yeah."}, {"block_idx": 68, "token_num": 25, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I mean, it's really interesting that these models are so bad at bug finding when just naively prompted to find a bug."}, {"block_idx": 69, "token_num": 5, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "They're incredibly poorly calibrated."}, {"block_idx": 70, "token_num": 5, "speaker_id": "speaker_2", "speaker_name": "Arvid", "text": "Even the smartest models."}, {"block_idx": 71, "token_num": 6, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "Exactly, even o1."}, {"block_idx": 72, "token_num": 6, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "How do you explain that?"}, {"block_idx": 73, "token_num": 6, "speaker_id": "speaker_0", "speaker_name": "Lex", "text": "Is there a good intuition?"}, {"block_idx": 74, "token_num": 41, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "I think these models are really strong reflection of the pre-training distribution, and I do think they generalize as the loss gets lower and lower, but I don't think the loss and the scale is quite…"}, {"block_idx": 75, "token_num": 15, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "The loss is low enough such that they're really fully generalizing on code."}, {"block_idx": 76, "token_num": 26, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "The things that we use these things for, the frontier models that they're quite good at, are really code generation and question answering."}, {"block_idx": 77, "token_num": 44, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And these things exist in massive quantities in pre-training with all of the code in GitHub on the scale of many, many trillions of tokens and questions and answers on things like stack overflow and maybe GitHub issues."}, {"block_idx": 78, "token_num": 46, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And so when you try to push one of these things that really don't exist very much online, like for example, the Cursor Tab objective of predicting the next edit given the edits done so far, the brittleness kind of shows."}, {"block_idx": 79, "token_num": 36, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "And then bug detection is another great example, where there aren't really that many examples of actually detecting real bugs and then proposing fixes and the models just kind of really struggle at it."}, {"block_idx": 80, "token_num": 35, "speaker_id": "speaker_3", "speaker_name": "Aman", "text": "But I think it's a question of transferring the model in the same way that you get this fantastic transfer from pre-trained models just on code in general to the Cursor Tab objective."}]}